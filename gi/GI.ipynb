{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/mojb2/Thesis/GI-PVI/venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import bayesfunc as bf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data import REGRESSION_CONFIG, RegressionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GI_Dense(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_input,\n",
    "                num_output,\n",
    "                num_inducing,\n",
    "                nonlinearity,\n",
    "                prior_scale_factor,\n",
    "                dtype,\n",
    "                name=\"global_inducing_fc\",\n",
    "                **kwargs):\n",
    "    \n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        \n",
    "        self.num_input = num_input + 1\n",
    "        self.num_output = num_output\n",
    "        self.num_inducing = num_inducing\n",
    "        self.prior_scale_factor = prior_scale_factor\n",
    "        \n",
    "        # Set nonlinearity for the layer\n",
    "        self.nonlinearity = (lambda x: x) if nonlinearity is None else \\\n",
    "                            getattr(nn, nonlinearity)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Set up prior mean, scale and distribution\n",
    "        self.prior_mean = t.zeros(\n",
    "            shape=(self.num_output, self.num_input),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.prior_scale = t.ones(\n",
    "            shape=(self.num_output, self.num_input),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.prior_scale /= self.num_input**0.5\n",
    "        self.prior_scale *= self.prior_scale_factor\n",
    "        \n",
    "        self.prior = t.distributions.MultivariateNormal(\n",
    "            loc=self.prior_mean,\n",
    "            # scale_diag=self.prior_scale # old tf code\n",
    "            scale_tril=self.prior_scale\n",
    "        )\n",
    "        \n",
    "        # Set up pseudo observation means and variances\n",
    "        self.pseudo_means = t.zeros(\n",
    "            shape=(self.num_inducing, self.num_output),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.pseudo_mean = t.Parameter(self.pseudo_means)\n",
    "        \n",
    "        self.pseudo_log_prec = t.zeros(\n",
    "            shape=(self.num_inducing,),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.pseudo_log_prec = t.Parameter(self.pseudo_log_prec)\n",
    "\n",
    "    @property\n",
    "    def pseudo_precision(self):\n",
    "        return t.math.exp(self.pseudo_log_precision)\n",
    "\n",
    "    def q_prec_cov_chols(self, Uin):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param Uin: inducing set U_in\n",
    "        \"\"\"\n",
    "        \n",
    "        phiU = self.nonlinearity(Uin)\n",
    "        pseudo_prec = t.math.exp(self.pseudo_log_prec)\n",
    "        \n",
    "        # Compute precision matrix of multivariate normal\n",
    "        phiT_lambda_phi = t.einsum(\"mi, m, mj -> ij\", phiU, pseudo_prec, phiU)\n",
    "        \n",
    "        q_prec = t.linalg.diag(self.prior_scale[0, :]**-2.) + phiT_lambda_phi\n",
    "        \n",
    "        # Compute cholesky of approximate posterior precision\n",
    "        q_prec_chol = t.linalg.cholesky(q_prec)\n",
    "        \n",
    "        # Compute cholesky of approximate posterior covariance\n",
    "        iq_prec_chol = t.linalg.triangular_solve(\n",
    "            q_prec_chol,\n",
    "            t.eye(q_prec_chol.shape[0]),\n",
    "            lower=True\n",
    "        )\n",
    "        \n",
    "        q_cov = t.matmul(iq_prec_chol.T, iq_prec_chol)\n",
    "        q_cov = q_cov + 1e-5 * t.eye(q_cov.shape[0])\n",
    "        q_cov_chol = t.linalg.cholesky(q_cov)\n",
    "        \n",
    "        return q_prec_chol, q_cov_chol\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03554a3b958796e9f601eb4c099ea8a724837ec686441e47caabab69f1671920"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

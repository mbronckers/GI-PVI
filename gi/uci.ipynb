{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import bayesfunc as bf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data.regression import RegressionDataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "matplotlib.style.use('default')\n",
    "plt.rcParams.update({'axes.titlesize': 'large', 'axes.labelsize': 'medium'})\n",
    "colors_hex = {'blue': '#1F77B4', 'orange': '#FF7F0E', 'green': '#2CA02C'}\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f931413b450>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda.is_available()\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "t.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to download, init and use a UCI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uci import UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f9234e6c4d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein = UCI('protein', 1, 10000)\n",
    "protein.trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ashman's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess_data import download_datasets, process_dataset, datasets, protein_config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading protein dataset\n",
      "protein dataset already exists!\n",
      "Input  shape: (45730, 9)\n",
      "Output shape: (45730, 1)\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"./data/uci\"\n",
    "download_datasets(root_dir='./data/uci', datasets={'protein': datasets['protein']})\n",
    "process_dataset(os.path.join(dir_path, \"protein\"), protein_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = lambda x: os.path.join(dir_path, \"protein\", x)\n",
    "\n",
    "X, y = np.load(data_dir(\"x.npy\")), np.load(data_dir(\"y.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DataLoaderConfig, HyperParamConfig\n",
    "from dataclasses import asdict\n",
    "from data import ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = HyperParamConfig(\n",
    "    name = \"Regression - UCI\",\n",
    "\n",
    "    # Dimensions\n",
    "    batch_size=128,\n",
    "    epochs = 1000,\n",
    "    input_dim = 9,\n",
    "    output_dim = 1,\n",
    "    hidden_units = 100,\n",
    "\n",
    "    # Training\n",
    "    elbo_samples = 5,\n",
    "    inference_samples = 10,\n",
    "    regression_likelihood_noise = 0.1,\n",
    "\n",
    "    # Optimiser\n",
    "    lr = 1e-3,\n",
    "    step_size = 100, \n",
    "    gamma = 0.10,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ListDataset(X, y)\n",
    "N = len(dataset)\n",
    "train_test_split = 0.8\n",
    "fraction = math.ceil(train_test_split*N)\n",
    "train, test = t.utils.data.random_split(dataset, [fraction, N-fraction])\n",
    "\n",
    "train_loader, test_loader = DataLoader(train, **(params.dataloader)), DataLoader(test, **(params.dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inducing set U_0\n",
    "inducing_batch = 100\n",
    "inducing_data, inducing_targets = next(iter(train_loader))\n",
    " \n",
    "# for classification\n",
    "# inducing_targets = (t.arange(num_classes) == inducing_targets[:, None]).float()\n",
    "\n",
    "# Initialize to first <inducing_batch> training samples\n",
    "if inducing_batch < inducing_data.shape[0]:\n",
    "    inducing_data = inducing_data[:inducing_batch]\n",
    "    inducing_targets = inducing_targets[:inducing_batch]\n",
    "\n",
    "# Fill up inducing set with random initialized points on [0,1] if batch smaller than num inducing pts\n",
    "if inducing_batch > inducing_data.shape[0]:\n",
    "    inducing_data = t.cat([inducing_data, t.randn(inducing_batch-inducing_data.shape[0], *inducing_data.shape[1:], dtype=inducing_data.dtype)], 0)\n",
    "    \n",
    "    inducing_targets = t.cat(\n",
    "        [inducing_targets, t.randn(inducing_batch - inducing_targets.shape[0], *inducing_targets.shape[1:], dtype=inducing_targets.dtype)], 0)\n",
    "in_features = inducing_data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesfunc.priors import InsanePrior, NealPrior\n",
    "\n",
    "optional_layer_params = {\n",
    "    # 'bias': True,\n",
    "    'prior': NealPrior, #InsanePrior,\n",
    "    #'inducing_targets': initial value of inducing targets (default None)\n",
    "    'log_prec_init': -4, # initial precision parameter values; default -4, assumes little data available\n",
    "    'log_prec_lr': 1, # LR multiplier for precision params\n",
    "    # 'inducing_targets': inducing_targets\n",
    "}\n",
    "\n",
    "hidden_units=50\n",
    "bnn_net = nn.Sequential(\n",
    "    bf.GILinear(in_features=params.input_dim, out_features=hidden_units, inducing_batch=inducing_batch, bias=True, **optional_layer_params),\n",
    "    nn.ReLU(),\n",
    "    bf.GILinear(in_features=hidden_units, out_features=hidden_units, inducing_batch=inducing_batch, bias=True, **optional_layer_params),\n",
    "    nn.ReLU(),\n",
    "    bf.GILinear(in_features=hidden_units, out_features=params.output_dim, inducing_batch=inducing_batch, bias=True, full_prec=True, **optional_layer_params)\n",
    ")\n",
    "\n",
    "# Wrap model in inducing wrapper\n",
    "dtype = t.float64\n",
    "# bnn_net = bf.InducingWrapper(bnn_net, inducing_shape=(inducing_batch, in_features), inducing_batch=inducing_batch)\n",
    "bnn_net = bf.InducingWrapper(bnn_net, inducing_data=inducing_data, inducing_batch=inducing_batch)\n",
    "# Send to GPU\n",
    "bnn_net = bnn_net.to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, epoch, scale, factor, num_samples=10, plot=False):\n",
    "    iters = 0\n",
    "    total_elbo = 0.\n",
    "    total_ll = 0.\n",
    "    total_KL = 0.\n",
    "    total_error = 0.\n",
    "\n",
    "    # Tempered beta to have loss not be KL divergence (to optimize eval performance)\n",
    "    temper = False\n",
    "    L = 1. # temperature scaling\n",
    "    tempered_beta = 0.1*math.floor((epoch-1)/10.)/L if (temper and epoch < 100) else 1/L\n",
    "    beta = 1/L\n",
    "\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        opt.zero_grad() # zero out the gradient (Torch accumulates across epochs otherwise)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.expand(num_samples, *data.shape) # make it S x N x D\n",
    "        \n",
    "        outputs, logPQw, _ = bf.propagate(net, data)\n",
    "        \n",
    "        # Classification: compute log likelihood estimate via outputs \n",
    "        # dist| = Categorical(logits=outputs)\n",
    "        # ll = dist.log_prob(target).mean() \n",
    "        \n",
    "        def log_s2():\n",
    "            return scale*factor\n",
    "\n",
    "        # Regression:\n",
    "        ll = Normal(outputs.to(device), t.exp(0.5*log_s2())).log_prob(target) # [10, batch size, output] => need to take mean across inference samples (first dim)\n",
    "        ll = ll.mean(0)\n",
    "        ll = ll.sum() # across batch size\n",
    "        \n",
    "        # Compute objectives\n",
    "        nloss = ll.mean() + tempered_beta * logPQw.mean()/ batch_size  # tempered ELBO\n",
    "        elbo = ll + beta * (logPQw / batch_size)\n",
    "\n",
    "        (-elbo.mean()).backward()  \n",
    "        opt.step()\n",
    "\n",
    "        # For classification only\n",
    "        # output = outputs.log_softmax(-1).logsumexp(0) - math.log(outputs.shape[0])\n",
    "        # pred = output.argmax(dim=-1, keepdim=True)\n",
    "        # correct = pred.eq(target.view_as(pred)).float().mean()\n",
    "        \n",
    "        error = (outputs.mean() - target)\n",
    "        \n",
    "        iters         += 1\n",
    "        total_elbo    += elbo.mean().item()\n",
    "        total_ll      += ll.mean().item()\n",
    "        total_KL      -= (beta*logPQw.mean()/batch_size).item()\n",
    "        total_error   += error.sum()\n",
    "\n",
    "    return (total_elbo/iters, total_ll/iters, total_KL/iters, total_error/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader, scale, num_samples=10):\n",
    "    iters = 0\n",
    "    total_elbo = 0.\n",
    "    total_ll = 0.\n",
    "    total_KL = 0.\n",
    "    total_error = 0.\n",
    "    \n",
    "    for data, target in loader:\n",
    "        batch_size = data.shape[0]\n",
    "                \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.expand(num_samples, *data.shape) # make it S x N x D\n",
    "        \n",
    "        outputs, logPQw, _ = bf.propagate(net, data)\n",
    "        \n",
    "        # Classification: compute log likelihood estimate via outputs \n",
    "        # dist| = Categorical(logits=outputs)\n",
    "        # ll = dist.log_prob(target).mean() \n",
    "        \n",
    "        def log_s2():\n",
    "            return scale*factor\n",
    "\n",
    "        # Regression:\n",
    "        ll = Normal(outputs.to(device), t.exp(0.5*log_s2())).log_prob(target) # [10, batch size, output] => need to take mean across inference samples (first dim)\n",
    "        ll = ll.mean(0)\n",
    "        ll = ll.sum() # across batch size\n",
    "        \n",
    "        # Compute objectives\n",
    "        elbo = ll + (logPQw / batch_size)\n",
    "        error = (outputs.mean() - target) # regression\n",
    "\n",
    "        # For classification only\n",
    "        # output = outputs.log_softmax(-1).logsumexp(0) - math.log(outputs.shape[0])\n",
    "        # pred = output.argmax(dim=-1, keepdim=True)\n",
    "        # correct = pred.eq(target.view_as(pred)).float().mean()\n",
    "        \n",
    "        iters         += 1\n",
    "        total_ll      += ll.mean().item()\n",
    "        total_error   += error.sum()\n",
    "\n",
    "    return (total_ll/iters, total_error/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 000, time:  4.16 (total 4.16), elbo: -51.023, KL: 5.206, train err: 0.201, test err: 1.709: 100%|██████████| 10/10 [00:41<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-52.25177021986619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = bnn_net\n",
    "loader = train_loader\n",
    "\n",
    "lr = 0.05\n",
    "factor=10\n",
    "log_s2_scaled = t.tensor(-3./factor, requires_grad=True, device=device)\n",
    "opt = t.optim.Adam([*net.parameters(), log_s2_scaled], lr=lr)\n",
    "scheduler = ExponentialLR(opt, gamma=0.95)\n",
    "epochs = 10\n",
    "\n",
    "epoch = []\n",
    "train_elbo = []\n",
    "train_ll = []\n",
    "train_KL = []\n",
    "test_ll = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "pbar = tqdm(range(epochs), position=0, leave=True)\n",
    "start_time = timer()\n",
    "prev_time = timer()\n",
    "for _epoch in pbar:\n",
    "    \n",
    "    _elbo, _train_ll, _train_KL, _train_err = train(net, loader, _epoch, log_s2_scaled, factor)\n",
    "\n",
    "    if _epoch % 100 == 0: scheduler.step()\n",
    "\n",
    "    epoch.append(_epoch)\n",
    "    train_elbo.append(_elbo)\n",
    "    train_ll.append(_train_ll)\n",
    "    train_KL.append(_train_KL)\n",
    "    train_err.append(_train_err)\n",
    "    \n",
    "\n",
    "    with t.no_grad():\n",
    "        _test_ll, _test_err = test(net, test_loader, log_s2_scaled)\n",
    "    test_ll.append(_test_ll)\n",
    "    test_err.append(_test_err)\n",
    "\n",
    "    if _epoch%10 == 0:\n",
    "        report_time = timer() - prev_time\n",
    "        prev_time += report_time\n",
    "        total_time = timer() - start_time\n",
    "\n",
    "        report = f\"epoch: {_epoch:03d}, time: {report_time: 3.2f} (total {total_time:3.2f}), elbo: {_elbo:.3f}, KL: {_train_KL:.3f}, train err: {_train_err:.3f}, test err: {_test_err:.3f}\"\n",
    "        pbar.set_description(report)\n",
    "    \n",
    "print(train_elbo[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(net, test_loader: DataLoader=None, num_inference_samples=10):\n",
    "    xs = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    stds = []\n",
    "\n",
    "    with t.no_grad(): # no grad computation\n",
    "\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            reshaped_data = data.expand(num_inference_samples, *data.shape) # make it S x N x D\n",
    "        \n",
    "            outputs, logPQw, sample_dict = bf.propagate(bnn_net, reshaped_data)\n",
    "            \n",
    "            pm = outputs.mean(0).flatten().cpu() \n",
    "            ps = outputs.std(0).flatten().cpu()\n",
    "\n",
    "            xs.append(data)\n",
    "            targets.append(target)\n",
    "            preds.append(pm)\n",
    "            stds.append(ps)\n",
    "\n",
    "\n",
    "        xs = t.cat(xs, dim=1).cpu() # shape: inference_samples x batch_size/N x 1\n",
    "        xs = xs.flatten()\n",
    "        preds = t.cat(preds, dim=0).cpu()\n",
    "        targets = t.cat(targets, dim=1).cpu() # shape: N x 1\n",
    "        targets = targets.flatten()\n",
    "        stds = t.cat(stds, dim=0).cpu()\n",
    "\n",
    "        # Sort in same order\n",
    "        xs, preds, targets, stds = (np.array(t) for t in zip(*sorted(zip(xs, preds, targets, stds))))\n",
    "\n",
    "        plt.fill_between(xs, preds-2*stds, preds+2*stds, alpha=0.5)\n",
    "        plt.plot(xs, preds, label='Prediction mean')\n",
    "        plt.scatter(xs, targets, label='True values')\n",
    "        plt.show()\n",
    "\n",
    "        return xs, preds, targets, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03554a3b958796e9f601eb4c099ea8a724837ec686441e47caabab69f1671920"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
